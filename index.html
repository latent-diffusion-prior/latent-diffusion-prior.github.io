<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <title>Text-driven Visual Synthesis with Latent Diffusion Prior Demo</title>
    <meta name="description" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="style/main.css">
    <link rel="stylesheet" href="style/glide.core.min.css">
    <link rel="stylesheet" href="style/glide.theme.min.css">
    <link rel="stylesheet" href="style/glide-custom.css">
  </head>
  <body onload="selectSource('swan'); selectAppVideo('human1'); selectComparisonVideo('parrot'); change_text_promt('gan-teaser', 0)">
    <div class="container" id="container">
      <div class="page page1">
        <h1 style="text-align: center">
          Text-driven Visual Synthesis with Latent Diffusion Prior</h1>
        <div class="authors">
          <div><a href="https://tinghliao.github.io/">Ting-Hsuan Liao</a></div>
          <div><a href="http://songweige.github.io/">Songwei Ge</a></div>
          <div><a href="https://twizwei.github.io/">Yiran Xu</a></div>
          <div><a href="https://yaochih.github.io/">Yao-Chih Lee</a></div>
          <div><a href="https://badouralbahar.github.io">Badour AlBahar</a></div>
          <div><a href="https://jbhuang0604.github.io">Jia-Bin Huang</a></div>
        </div>
        <p style="text-align: center">University of Maryland, College Park</p>
        <br/>
        <div class="links">
          <a class="btn-solid-lg" href="#"><i class="fa fa-file-pdf-o"></i> Paper (arXiv)</a>
          <a class="btn-solid-lg" href="#"><i class="fa fa-github"></i> Code (coming soon)</a>
        </div>

          <div class="glide" id="gan-teaser">
            <div data-glide-el="track" class="glide__track">
              <ul class="glide__slides" id="gan-teaser-sub">
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-0">
                    <source src="videos/adaptation/teaser-human1.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-1">
                    <source src="videos/adaptation/teaser-human2.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-2">
                    <source src="videos/adaptation/teaser-cat1.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-3">
                    <source src="videos/adaptation/teaser-cat2.mp4" type="video/mp4"/>
                  </video>
                </li>
              </ul>
            </div>

            <div class="glide__arrows" data-glide-el="controls">
              <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
              <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
            </div>
            <p class="start prompt" id="teaser-gan"><b>Text prompt:</b> A very beautiful anime girl, full body, long braided curly silver hair, sky blue eyes, full round face, short smile, casual clothes, ice snowy lake setting, cinematic lightning, medium shot, mid-shot, highly detailed, trending on Artstation, Unreal Engine 4k, cinematic wallpaper by Stanley Artgerm Lau, WLOP, Rossdraws, James Jean, Andrei Riabovitchev, Marc Simonetti, and Sakimichan.</p>
            <p class="center shift-to-above">StyleGAN Adaptation.</p>
          </div>
        </br></br>

          <div class="teaser">
            <div style="display: flex;flex-direction: row;">
              <p id="comparison-caption" style="margin-left: 7%; margin-top: 5px; text-align: center; width: 50%">Jacobian NeRF</p>
              <p id="comparison-caption-our" style="margin-right: 7%; margin-top: 5px; text-align: center; width: 50%;">Jacobian NeRF + Ours<mark style="background: none; color: white">.</mark></p>
            </div>
            <div class="glide" id="text3d-teaser">
              <div data-glide-el="track" class="glide__track">
                <ul class="glide__slides">
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-0">
                      <source src="videos/jog.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-1">
                      <source src="videos/fish.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-2">
                      <source src="videos/rose.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-3">
                      <source src="videos/straw.mp4" type="video/mp4"/>
                    </video>
                  </li>
                </ul>
              </div>
  
              <div class="glide__arrows" data-glide-el="controls">
                <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
                <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
              </div>
            </div>
            <p class="start prompt" id="teaser-text"><b>Text prompt:</b> A high quality photo of a jug made of blue and white porcelain.</p>
            <p class="center shift-to-above">Text-to-3D compare with <a href="https://pals.ttic.edu/p/score-jacobian-chaining">Jacobian-NeRF</a>, and <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a>.</p>
          </br></br>

          <div class="glide" id="layer-teaser">
            <div data-glide-el="track" class="glide__track">
              <ul class="glide__slides" id="layer-teaser-sub">
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-0">
                    <source src="videos/layer/teaser_horse.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-1">
                    <source src="videos/layer/teaser_bear.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-2">
                    <source src="videos/layer/teaser_bread.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-3">
                    <source src="videos/layer/teaser_cat.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-4">
                    <source src="videos/layer/teaser_swan.mp4" type="video/mp4"/>
                  </video>
                </li>
              </ul>
            </div>

            <div class="glide__arrows" data-glide-el="controls">
              <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
              <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
            </div>
          </div>
          <p class="start prompt" id="teaser-layer"><b>Text prompt:</b> Golden Horse.</p>
          <p class="center shift-to-above">Layer image editing.</p>
        </div>
      </div>
    </div>

    <div class="container">
      <h2>Comparison: StyleGAN Adaptation</h2>
      <!-- <p>
        Our proposed method can also be applied to image generator adaptation. We conduct experiments on StyleGAN2 with our feature matching loss. To
        demonstrate the effectiveness of our method, we compare our approach with two other baselines, StyleGANFusion and StyleGAN-NADA. 
      </p> -->
      <!-- <img src="figures/cat.png" style="width: 100%"> -->
      <div style="display: block;" id="application-div">
        <video id="app-player" autoplay muted onended="loadApp(true)">
          <source id="app-src" src="">
        </video>
      </div>
      <div style="display: flex;flex-direction: row;">
        <p style="margin-top: -3px; text-align: center; width: 25%">Source</p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">StyleGAN-NADA <br> [Gal et al. 2022]</p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">StyleGANFusion <br>[Song et al. 2022] </p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">Ours</p>
      </div>
      <p class="start prompt" id="com-gan"><b>Text prompt:</b> Photo of a face [SEP] A realistic detailed portrait, single face, science fiction, artstation, volumetric lighting, octane render.</p>
      <p><b>Example</b></p>
      <div class="btn-group btn-group-app">
        <button class='btn-app-video' onclick="selectAppVideo('human1'), change_text_promt('com-gan', 0)">Human Face 1</button>
        <button class='btn-app-video' onclick="selectAppVideo('human2'), change_text_promt('com-gan', 1)">Human Face 2</button>
        <button class='btn-app-video'  onclick="selectAppVideo('cat1'), change_text_promt('com-gan', 2)">Cat 1</button>
        <button class='btn-app-video'  onclick="selectAppVideo('cat2'), change_text_promt('com-gan', 3)">Cat 2</button>
        <button class='btn-app-video'  onclick="selectAppVideo('cat3'), change_text_promt('com-gan', 4)">Cat 3</button>
        <!-- &#8594 -->
      </div>
      <br/>
      <img src="figures/compare.png" style="display: block; width: 90%; margin: 0 auto;"/>
      <p class="center shift-to-above">Compare our method with StyleGAN-NADA [Gal et al. 2022] and StyleGANFusion [Song et al. 2022] on FID (left, the lower the better) and LPIPS/CLIP (right, the higher the better) score.</p>
    </div>

    <div class="container" id="comparison-video">
      <h2>Comparison: Text-to-3D</h2>
        <div style="display: block;" id="comparison-video-div">
          <video id="comparison-player1"  style="width: 49%" autoplay muted onended="">
            <source id="comparison-src1" src="videos/text3D/skate.mp4">
          </video>
          <video id="comparison-player2"  style="width: 49%" autoplay muted onended="loadComparison()">
            <source id="comparison-src2" src="videos/text3D/skate.mp4">
          </video>
        </div>
        <div style="display: flex;flex-direction: row;">
          <p id="comparison-caption" style="margin-top: 5px; text-align: center; width: 50%">Jacobian NeRF <br> [Wang et al. 2022]</p>
          <p style="margin-top: 5px; text-align: center; width: 50%;">Ours<br/><mark style="background: none; color: white">.</mark></p>
        </div>
        <p class="start prompt" id="com-text"><b>Text prompt:</b> duck</p>
        <p><b>Example</b></p>
        <div class="btn-group btn-group-comp-video">
          <button class='btn-comp-video'  onclick="selectComparisonVideo('parrot')">parrot</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('skate')">skate</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('bird')">bird</button>
          <button class='btn-comp-video' onclick="selectComparisonVideo('boat')">boat</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('ice')">ice cream</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('lego')">lego</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('water')">fire hydrant</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('wine')">red wine</button>
        </div>
        <br/><br/>
      </div>
    </div>

    <div class="container">
      <h2>Comparison: Layered Editing</h2>
      <!-- <p>
        We demonstrate the application of our diffusion prior to the image editing task. Unlike existing diffusion-based editors, our method manipulates images using test-time optimization with the proposed diffusion
        guidance. The results reveal that our method produces more detailed results than the Dreamfusion-guided baseline and the CLIP-guided method, Text2LIVE. 
      </p> -->
        <div class="cocoen" style="width: 100%;">
          <img id="image-player1"  src="figures/swan/source.jpg" alt="" />
          <img id="image-player2"  src="figures/swan/FM.png" alt="" />
        </div>
        <p style="margin-top: 5px; text-align: center;">Slide the bar to compare input (left) and output (right)</p>
        <p><b>Source Image</b></p>
        <div class="btn-group">
          <button class='btn-img' style="width:23%" onclick="selectSource('swan')">Swan &#8594 White Swan</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('horse')">Horse &#8594 Zebra</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('cat')">Cat &#8594 Tiger</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('burger')">Bread &#8594 Sub Sandwich</button>
        </div>
        <p><b>Method</b></p>
        <div class="btn-group">
            <button class='btn-mth' style="width:23%" onclick="selectMethod(1)">Text2LIVE</button>
            <button class='btn-mth' style="width:23%" onclick="selectMethod(2)">Ours-base</button>
            <button class='btn-mth' style="width:23%" onclick="selectMethod(3)">Ours-FM</button>
        </div>
      </div>
    </div>


    <script
      src="https://code.jquery.com/jquery-3.2.1.js"
      integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE="
      crossorigin="anonymous"></script>
    <script src="js/interactive_demo.js"></script>
    <script src="dist/cocoen.js"></script>
    <script src="dist/glide.min.js"></script>
    <script>
      Cocoen.parse(document.body);
    </script>
    
    <script>
        glide = new Glide('#text3d-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        // let video_num = 4;
        glide.on('run', () => {
          var index = glide.index;
          changecaption(index);
          change_text_promt('teaser-text', index)
        //   for(var i = 0; i < video_num; i++) {
        //     if(i == index) {
        //       var active_videos = document.getElementsByClassName('teaser-video-' + i);
        //       for(var j = 0; j < active_videos.length; j++) {
        //         var video_ele = active_videos[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           changecaption(index%2*2);
        //           change_text_promt('teaser-text', i)
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos = document.getElementsByClassName('teaser-video-' + i);
        //       for(var j = 0; j < inactive_videos.length; j++) {
        //         inactive_videos[j].pause(); 
        //       }
        //     }
        //   }
        });
        glide_layer = new Glide('#layer-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        glide_layer.on('run', () => {
          var index = glide_layer.index;
          change_text_promt('teaser-layer', index)
        });
        // let video_num_layer = 5;
        // glide_layer.on('run', () => {
        //   var index = glide_layer.index;
        //   for(var i = 0; i < video_num_layer; i++) {
        //     if(i == index) {
        //       var active_videos_layer = document.getElementsByClassName('teaser-video-layer-' + i);
        //       // console.log(active_videos_layer.length);
        //       for(var j = 0; j < active_videos_layer.length; j++) {
        //         var video_ele = active_videos_layer[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           change_text_promt('teaser-layer', i)
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos_layer = document.getElementsByClassName('teaser-video-layer-' + i);
        //       for(var j = 0; j < inactive_videos_layer.length; j++) {
        //         inactive_videos_layer[j].pause(); 
        //       }
        //     }
        //   }
        // });
        glide_gan = new Glide('#gan-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        glide_gan.on('run', () => {
          var index = glide_gan.index;
          change_text_promt('gan-teaser', index)
        });
        // let video_num_gan = 4;
        // glide_gan.on('run', () => {
        //   var index = glide_gan.index;
        //   for(var i = 0; i < video_num_gan; i++) {
        //     if(i == index) {
        //       var active_videos_gan = document.getElementsByClassName('teaser-video-gan-' + i);
        //       for(var j = 0; j < active_videos_gan.length; j++) {
        //         var video_ele = active_videos_gan[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           change_text_promt('gan-teaser', i);
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos_gan = document.getElementsByClassName('teaser-video-gan-' + i);
        //       for(var j = 0; j < inactive_videos_gan.length; j++) {
        //         inactive_videos_gan[j].pause(); 
        //       }
        //     }
        //   }
        // });
    </script>
  <div class="container">
    <h2>Abstract</h2>
    <p style="text-align: justify;">There has been tremendous progress in large-scale text-to-image synthesis driven by 
      diffusion models enabling versatile downstream applications such as 3D object synthesis from texts, image editing 
      and generation. We present a generic approach using latent diffusion models as powerful image priors for various 
      visual synthesis tasks. Existing methods that utilize such priors fail to use these models' full capabilities. 
      To improve this, our core ideas are 
      <ul>
        <li>a feature matching Loss between features from different layers of the decoder 
          to provide detailed guidance</li>
        <li>a KL divergence loss to regularize the predicted latent features and stabilize the 
          training.</li>
      </ul>
      We demonstrate the efficacy of our approach on three different applications, text-to-3D, StyleGAN adaptation, 
      and layered image editing. Extensive results show our method compares favorably against baselines.</p>
    <h2 style="margin-top: 40px;">Overview</h2>
    <img src="figures/Overview.jpg" style="width: 100%">
    <p style="text-align: justify;">Our method guides the generation and editing given a text prompt. We obtain the latent code from a differentiable
      renderer under different applications, including the generator form Text2LIVE [Bar-Tal et al. 2022], StyleGAN-based generator, or a NeRF model. 
      This latent code ùë£ is perturbed following the latent diffusion model‚Äôs scheduler at arandom timestep ùë°, such that ùêπ<small>ùë°</small>: z<small>ùë°</small> = ùõº<small>ùë°</small>ùë£ + ùúé<small>ùë°</small>ùúñ. This perturbed 
      latent code z<small>ùë°</small> is then passed to the UNet to generate the predicted noise ùúñÀÜ. We then use the predicted noise ùúñÀÜ to derive the latent score distillation 
      gradient. To derive the feature matching gradient, we input the latent code ùë£ and noised latent code ùë£ + (ùúñÀÜ ‚àí ùúñ) into the decoder ùê∫<small>ùúô<small>ùëëùëíùëê</small></small>(¬∑). We
      compute the difference between the multi-level features from three different layers of the decoder to compute the feature matching loss. 
      Finally, both the latent score distillation and multi-level feature matching gradients are backpropagated to the differentiable renderer.
    </p>
  </div>
  <div class="container">
  <h2>BibTeX</h2>
  <pre><code>
    @article{liao2023textsyndiffusionprior,
      title   = {Text-driven Visual Synthesis with Latent Diffusion Prior},
      author  = {Liao, Ting-Hsuan and Ge Songwei and Xu Yiran and Lee, Yao-Chih and AlBahar Badour and Huang, Jia-Bin},
      journal = {arXiv preprint arXiv:},
      year    = {2023}
    }    
  }</code></pre>
  
  <h2>Acknowledgements</h2>
  <p>We thank Jacobian NeRF, Latent NeRF, Text2Live and StyleGANFusion authors.</p>
  </div>
  </body>
</html>

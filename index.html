<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <title>Text-driven Visual Synthesis with Latent Diffusion Prior Demo</title>
    <meta name="description" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="style/main.css">
    <link rel="stylesheet" href="style/glide.core.min.css">
    <link rel="stylesheet" href="style/glide.theme.min.css">
    <link rel="stylesheet" href="style/glide-custom.css">
  </head>
  <body onload="selectSource('swan'); selectAppVideo('badger'); selectComparisonVideo('parrot'); change_text_promt('gan-teaser', 0)">
    <div class="container" id="container">
      <div class="page page1">
        <h1 style="text-align: center; font-size: 30px;"> </br>
          Text-driven Visual Synthesis with Latent Diffusion Prior</h1>
        <div class="authors ">
          <div><a href="https://tinghliao.github.io/">Ting-Hsuan Liao</a></div>
          <div><a href="http://songweige.github.io/">Songwei Ge</a></div>
          <div><a href="https://twizwei.github.io/">Yiran Xu</a></div>
          <div><a href="https://yaochih.github.io/">Yao-Chih Lee</a></div>
          <div><a href="https://badouralbahar.github.io">Badour AlBahar</a></div>
          <div><a href="https://jbhuang0604.github.io">Jia-Bin Huang</a></div>
        </div>
        <p style="text-align: center">University of Maryland, College Park</p>
        <br/>
        <div class="links">
          <a class="btn-solid-lg" href="https://arxiv.org/abs/2302.08510"><i class="fa fa-file-pdf-o"></i> Paper (arXiv)</a>
          <a class="btn-solid-lg" href="#"><i class="fa fa-github"></i> Code (coming soon)</a>
        </div>
        <div class="teaser">
          <div style="display: flex;flex-direction: row;">
            <p style="margin-left: 7%; margin-top: 5px; text-align: center; width: 50%">Input</p>
            <p style="margin-right: 7%; margin-top: 5px; text-align: center; width: 50%;">Ours<mark style="background: none; color: white">.</mark></p>
          </div>
          <div class="glide" id="gan-teaser">
            <div data-glide-el="track" class="glide__track">
              <ul class="glide__slides" id="gan-teaser-sub">
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-0">
                    <source src="videos/adaptation/teaser-human1.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-1">
                    <source src="videos/adaptation/teaser-human2.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-2">
                    <source src="videos/adaptation/teaser-cat1.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-gan-3">
                    <source src="videos/adaptation/teaser-cat2.mp4" type="video/mp4"/>
                  </video>
                </li>
              </ul>
            </div>

            <div class="glide__arrows" data-glide-el="controls">
              <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
              <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
            </div>
            <p class="start prompt" id="teaser-gan"><b>Text prompt:</b> A very beautiful anime girl, full body, long braided curly silver hair, sky blue eyes, full round face, short smile, casual clothes, ice snowy lake setting, cinematic lightning, medium shot, mid-shot, highly detailed, trending on Artstation, Unreal Engine 4k, cinematic wallpaper by Stanley Artgerm Lau, WLOP, Rossdraws, James Jean, Andrei Riabovitchev, Marc Simonetti, and Sakimichan.</p>
            <p class="center" style="font-style: italic;">StyleGAN Adaptation.</p>
          </div>
          </br></br>
            <div style="display: flex;flex-direction: row;">
              <p id="comparison-caption" style="margin-left: 7%; margin-top: 5px; text-align: center; width: 50%">Jacobian NeRF</p>
              <p id="comparison-caption-our" style="margin-right: 7%; margin-top: 5px; text-align: center; width: 50%;">Jacobian NeRF + Ours<mark style="background: none; color: white">.</mark></p>
            </div>
            <div class="glide" id="text3d-teaser">
              <div data-glide-el="track" class="glide__track">
                <ul class="glide__slides">
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-0">
                      <source src="videos/jog.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-1">
                      <source src="videos/fish.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-2">
                      <source src="videos/rose.mp4" type="video/mp4"/>
                    </video>
                  </li>
                  <li class="glide__slide">
                    <video controls muted loop autoplay class="teaser-video-3">
                      <source src="videos/straw.mp4" type="video/mp4"/>
                    </video>
                  </li>
                </ul>
              </div>
  
              <div class="glide__arrows" data-glide-el="controls">
                <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
                <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
              </div>
            </div>
            <p class="start prompt" id="teaser-text"><b>Text prompt:</b> A high quality photo of a jug made of blue and white porcelain.</p>
            <p class="center shift-to-above">Text-to-3D compare with <a href="https://pals.ttic.edu/p/score-jacobian-chaining">Jacobian-NeRF</a>, and <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a>.</p>
          </br></br>

          <div class="glide" id="layer-teaser">
            <div data-glide-el="track" class="glide__track">
              <ul class="glide__slides" id="layer-teaser-sub">
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-0">
                    <source src="videos/layer/teaser_horse.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-1">
                    <source src="videos/layer/teaser_bear.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-2">
                    <source src="videos/layer/teaser_bread.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-3">
                    <source src="videos/layer/teaser_cat.mp4" type="video/mp4"/>
                  </video>
                </li>
                <li class="glide__slide">
                  <video controls muted loop autoplay class="teaser-video-layer-4">
                    <source src="videos/layer/teaser_swan.mp4" type="video/mp4"/>
                  </video>
                </li>
              </ul>
            </div>

            <div class="glide__arrows" data-glide-el="controls">
              <span class="glide__arrow glide__arrow--left" data-glide-dir="<"><img src="dist/arrow-left-circle-fill.svg"/></span>
              <span class="glide__arrow glide__arrow--right" data-glide-dir=">"><img src="dist/arrow-right-circle-fill.svg"/></span>
            </div>
          </div>
          <p class="start prompt" id="teaser-layer"><b>Text prompt:</b> Golden Horse.</p>
          <p class="center shift-to-above">Layer image editing.</p>
        </div>
      </div>
    </div>

    <div class="container justify">
      <h2>Abstract</h2>
      <p style="text-align: justify;">There has been tremendous progress in large-scale text-to-image synthesis driven by 
        diffusion models enabling versatile downstream applications such as 3D object synthesis from texts, image editing 
        and generation. We present a generic approach using latent diffusion models as powerful image priors for various 
        visual synthesis tasks. Existing methods that utilize such priors fail to use these models' full capabilities. 
        To improve this, our core ideas are 
        <ul>
          <li>a feature matching Loss between features from different layers of the decoder 
            to provide detailed guidance</li>
          <li>a KL divergence loss to regularize the predicted latent features and stabilize the 
            training.</li>
        </ul>
        We demonstrate the efficacy of our approach on three different applications, text-to-3D, StyleGAN adaptation, 
        and layered image editing. Extensive results show our method compares favorably against baselines.</p>
      <h2 style="margin-top: 40px;">Overview</h2>
      <img src="figures/Overview.jpg" style="width: 80%;  margin-left: 10%;">
      <p style="text-align: justify;">Our method guides the generation and editing given a text prompt. We obtain the latent code from a differentiable
        renderer under different applications. 
        This latent code ùë£ is perturbed following the latent diffusion model‚Äôs scheduler at a random timestep ùë°, such that ùêπ<small>ùë°</small>: z<small>ùë°</small> = ùõº<small>ùë°</small>ùë£ + ùúé<small>ùë°</small>ùúñ. This perturbed 
        latent code z<small>ùë°</small> is then passed to the UNet to generate the predicted noise ùúñÀÜ. We then use the predicted noise ùúñÀÜ to derive the latent score distillation 
        gradient. To derive the feature matching gradient, we input the latent code ùë£ and noised latent code ùë£ + (ùúñÀÜ ‚àí ùúñ) into the decoder ùê∫<small>ùúô<small>ùëëùëíùëê</small></small>(¬∑). We
        compute the difference between the multi-level features from three different layers of the decoder to compute the feature matching loss. 
        Finally, both the latent score distillation and multi-level feature matching gradients are backpropagated to the differentiable renderer.
      </p>
      <h2 style="margin-top: 10px;">Method</h2>
      <img src="figures/applcation_overview.png" style="width: 100%;">
      <div style="display: flex;flex-direction: row;">
        <p style="margin-left: 0%; margin-top: 3px; text-align: center; width: 23%">(a) Text-to-3D</p>
        <p style="margin-left: 7%; margin-top: 3px; text-align: center; width: 30%">(b) StyleGAN adaptation</p>
        <p style="margin-right: 7%; margin-top: 3px; text-align: center; width: 45%;">(c) Layered image editing<mark style="background: none; color: white">.</mark></p>
      </div>
      <p>To apply our proposed method,
        we first obtain the latent code ùë£ using the differentiable renderer in each application. As illustrated in the above figure, 
        to obtain the image that produces ùë£ with StableDiffusion encoder ùê∏<small>ùúô<small>ùëëùëíùëê</small></small> , 
        in (a) Text-to-3D, we render from a NeRF model with a random camera viewpoint; (b) StyleGAN adaptation, 
        we generate the image with a pretrained StyleGAN model; (c) Layered image editing application, we use the generator of Text2LIVE to synthesize the edited image, 
        alpha map, and the alpha blending of the initial and edited images.</p>
    </div>


    <div class="container">
      <h2>Comparison: StyleGAN Adaptation</h2>
      <!-- <p>
        Our proposed method can also be applied to image generator adaptation. We conduct experiments on StyleGAN2 with our feature matching loss. To
        demonstrate the effectiveness of our method, we compare our approach with two other baselines, StyleGANFusion and StyleGAN-NADA. 
      </p> -->
      <!-- <img src="figures/cat.png" style="width: 100%"> -->
      <div class="row">
        <div class="col4">
          <img id="img1" src="figures/GAN/badger-original.png" style="width:95%">
        </div>
        <div class="col4">
          <img id="img2" src="figures/GAN/badger-nada.png"  style="width:95%">
        </div>
        <div class="col4">
          <img id="img3" src="figures/GAN/badger-stylediffuse.png" style="width:95%">
        </div>
        <div class="col4">
        <img id="img4" src="figures/GAN/badger-ours.png" style="width:95%">
        </div>
      </div>
      <div style="display: flex;flex-direction: row;">
        <p style="margin-top: -3px; text-align: center; width: 25%">Source</p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">StyleGAN-NADA <br> [Gal et al. 2022]</p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">StyleGANFusion <br>[Song et al. 2022] </p>
        <p style="margin-top: -3px; text-align: center; width: 25%;">Ours</p>
      </div>
      <p class="start prompt" id="com-gan"><b>Text prompt:</b> Photo of a face [SEP] A realistic detailed portrait, single face, science fiction, artstation, volumetric lighting, octane render.</p>
      <p><b>Example</b></p>
      <div class="btn-group">
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('badger')">Badger</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('bear')">Bear</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('fox')">Fox</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('hamster')">Hamster</button>
      </div> 
      <div class="btn-group">
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('id9')">Prompt1*</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('id10')">Prompt2**</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('lion')">Lion</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('otter')">Otter</button>
      </div> 
      <div class="btn-group">
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('pig')">Pig</button>
        <button class='btn-app-video' style="width:23%" onclick="selectAppVideo('tiger')">Tiger</button>
      </div>       
      </br></br>
      <!-- <br/> -->
      <img src="figures/compare.png" style="display: block; width: 90%; margin: 0 auto;"/>
      <p class="center shift-to-above">Compare our method with StyleGAN-NADA [Gal et al. 2022] and StyleGANFusion [Song et al. 2022] on FID (left, the lower the better) and LPIPS/CLIP (right, the higher the better) score.</p>
      <small>*Prompt1: "3d cute cat, closeup cute and adorable, cute big circular reflective eyes, long fuzzy fur, Pixar render, unreal engine cinematic smooth, intricate detail, cinematic"
      </br> **Ptompt2: "A beautiful portrait of a cute cat. character design by cory loftis, fenghua zhong, ryohei hase, ismail inceoglu and ruan jia. artstation, volumetric light, detailed, photorealistic, fantasy, rendered in octane"</small>
      </br></br>
    </div>

    <div class="container" id="comparison-video">
      <h2>Comparison: Text-to-3D</h2>
        <div style="display: block;" id="comparison-video-div">
          <video id="comparison-player1"  style="width: 49%" autoplay muted onended="">
            <source id="comparison-src1" src="videos/text3D/skate.mp4">
          </video>
          <video id="comparison-player2"  style="width: 49%" autoplay muted onended="loadComparison()">
            <source id="comparison-src2" src="videos/text3D/skate.mp4">
          </video>
        </div>
        <div style="display: flex;flex-direction: row;">
          <p id="comparison-caption" style="margin-top: 5px; text-align: center; width: 50%">Jacobian NeRF <br> [Wang et al. 2022]</p>
          <p style="margin-top: 5px; text-align: center; width: 50%;">Ours<br/><mark style="background: none; color: white">.</mark></p>
        </div>
        <p class="start prompt" id="com-text"><b>Text prompt:</b> duck</p>
        <p><b>Example</b></p>
        <div class="btn-group btn-group-comp-video">
          <button class='btn-comp-video'  onclick="selectComparisonVideo('parrot')">parrot</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('skate')">skate</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('bird')">bird</button>
          <button class='btn-comp-video' onclick="selectComparisonVideo('boat')">boat</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('ice')">ice cream</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('lego')">lego</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('water')">fire hydrant</button>
          <button class='btn-comp-video'  onclick="selectComparisonVideo('wine')">red wine</button>
        </div>
        <br/><br/>
      </div>
    </div>

    <div class="container">
      <h2>Comparison: Layered Editing</h2>
      <!-- <p>
        We demonstrate the application of our diffusion prior to the image editing task. Unlike existing diffusion-based editors, our method manipulates images using test-time optimization with the proposed diffusion
        guidance. The results reveal that our method produces more detailed results than the Dreamfusion-guided baseline and the CLIP-guided method, Text2LIVE. 
      </p> -->
        <div class="cocoen" style="width: 100%;">
          <img id="image-player1"  src="figures/swan/source.jpg" alt="" />
          <img id="image-player2"  src="figures/swan/FM.png" alt="" />
        </div>
        <p style="margin-top: 5px; text-align: center;">Slide the bar to compare input (left) and output (right)</p>
        <p><b>Source Image</b></p>
        <div class="btn-group">
          <button class='btn-img' style="width:23%" onclick="selectSource('swan')">Swan &#8594 White Swan</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('horse')">Horse &#8594 Zebra</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('cat')">Cat &#8594 Tiger</button>
          <button class='btn-img' style="width:23%" onclick="selectSource('burger')">Bread &#8594 Sub Sandwich</button>
        </div>
        <p><b>Method</b></p>
        <div class="btn-group">
            <button class='btn-mth' style="width:23%" onclick="selectMethod(1)">Text2LIVE</button>
            <button class='btn-mth' style="width:23%" onclick="selectMethod(2)">Ours-base</button>
            <button class='btn-mth' style="width:23%" onclick="selectMethod(3)">Ours-FM</button>
        </div>
      </div>
    </div>


    <script
      src="https://code.jquery.com/jquery-3.2.1.js"
      integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE="
      crossorigin="anonymous"></script>
    <script src="js/interactive_demo.js"></script>
    <script src="dist/cocoen.js"></script>
    <script src="dist/glide.min.js"></script>
    <script>
      Cocoen.parse(document.body);
    </script>
    
    <script>
        glide = new Glide('#text3d-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        // let video_num = 4;
        glide.on('run', () => {
          var index = glide.index;
          changecaption(index);
          change_text_promt('teaser-text', index)
        //   for(var i = 0; i < video_num; i++) {
        //     if(i == index) {
        //       var active_videos = document.getElementsByClassName('teaser-video-' + i);
        //       for(var j = 0; j < active_videos.length; j++) {
        //         var video_ele = active_videos[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           changecaption(index%2*2);
        //           change_text_promt('teaser-text', i)
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos = document.getElementsByClassName('teaser-video-' + i);
        //       for(var j = 0; j < inactive_videos.length; j++) {
        //         inactive_videos[j].pause(); 
        //       }
        //     }
        //   }
        });
        glide_layer = new Glide('#layer-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        glide_layer.on('run', () => {
          var index = glide_layer.index;
          change_text_promt('teaser-layer', index)
        });
        // let video_num_layer = 5;
        // glide_layer.on('run', () => {
        //   var index = glide_layer.index;
        //   for(var i = 0; i < video_num_layer; i++) {
        //     if(i == index) {
        //       var active_videos_layer = document.getElementsByClassName('teaser-video-layer-' + i);
        //       // console.log(active_videos_layer.length);
        //       for(var j = 0; j < active_videos_layer.length; j++) {
        //         var video_ele = active_videos_layer[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           change_text_promt('teaser-layer', i)
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos_layer = document.getElementsByClassName('teaser-video-layer-' + i);
        //       for(var j = 0; j < inactive_videos_layer.length; j++) {
        //         inactive_videos_layer[j].pause(); 
        //       }
        //     }
        //   }
        // });
        glide_gan = new Glide('#gan-teaser', {
          type: "carousel",
          perView: 1.27,
          focusAt: "center",
          autoplay: 5500,
          hoverpause: true
        }).mount();
        glide_gan.on('run', () => {
          var index = glide_gan.index;
          change_text_promt('gan-teaser', index)
        });
        // let video_num_gan = 4;
        // glide_gan.on('run', () => {
        //   var index = glide_gan.index;
        //   for(var i = 0; i < video_num_gan; i++) {
        //     if(i == index) {
        //       var active_videos_gan = document.getElementsByClassName('teaser-video-gan-' + i);
        //       for(var j = 0; j < active_videos_gan.length; j++) {
        //         var video_ele = active_videos_gan[j];
        //         var isPlaying = video_ele.currentTime > 0 && !video_ele.paused && !video_ele.ended 
        //           && video_ele.readyState > video_ele.HAVE_CURRENT_DATA;

        //         if (!isPlaying) {
        //           video_ele.play();
        //           change_text_promt('gan-teaser', i);
        //         }
        //         video_ele.load();
        //         video_ele.currentTime = 0;
        //         video_ele.play();
        //       }
        //     } else {
        //       var inactive_videos_gan = document.getElementsByClassName('teaser-video-gan-' + i);
        //       for(var j = 0; j < inactive_videos_gan.length; j++) {
        //         inactive_videos_gan[j].pause(); 
        //       }
        //     }
        //   }
        // });
    </script>

  <div class="container">
  <h2>BibTeX</h2>
  <pre><code>
    @article{liao2023textsyndiffusionprior,
      title   = {Text-driven Visual Synthesis with Latent Diffusion Prior},
      author  = {Liao, Ting-Hsuan and Ge Songwei and Xu Yiran and Lee, Yao-Chih and AlBahar Badour and Huang, Jia-Bin},
      journal = {arXiv preprint arXiv:},
      year    = {2023}
    }    
  </code></pre>
  
  <h2>Acknowledgements</h2>
  <p>We thank <a href="https://pals.ttic.edu/p/score-jacobian-chaining">Jacobian-NeRF</a>, 
    <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a>,  
    <a href="https://text2live.github.io">Text2Live</a> and 
    <a href="https://styleganfusion.github.io">StyleGANFusion</a> authors.</p>
</br></br>
  </div>
  </body>
</html>
